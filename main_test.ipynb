{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Basic lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# config\n",
    "import yaml\n",
    "\n",
    "# Custom lib\n",
    "from src.loader import Loader\n",
    "from src.loan_preprocessor import Loan_Preprocessor\n",
    "from src.log_preprocessor import Log_Preprocessor\n",
    "from src.user_preprocessor import User_Preprocessor\n",
    "from src.cofix_preprocessor import Cofix_Preprocessor\n",
    "from src.matcher import Matcher\n",
    "\n",
    "# ignore warnings\n",
    "pd.set_option('mode.chained_assignment',  None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", 'r') as config_file:\n",
    "    config_dict = yaml.load(config_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_config = config_dict.get('Loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_config = config_dict.get('Loader')\n",
    "loader = Loader(loader_config)\n",
    "loan_df = loader.run('loan_result.csv')\n",
    "log_df = loader.run('log_data.csv')\n",
    "user_df = loader.run('user_spec.csv')\n",
    "cofix_df = loader.run('cofix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필요 없는 열 삭제 중...\n",
      "datetime으로 바꾸는 중...\n",
      "✅ prep dataset saved at (data\\prep\\prep_loan.fth)\n"
     ]
    }
   ],
   "source": [
    "prep_config = config_dict.get('Preprocessor')\n",
    "\n",
    "# Loan preprocess\n",
    "loan_preprocessor = Loan_Preprocessor(loan_df, prep_config)\n",
    "prep_loan_df = loan_preprocessor.run(\n",
    "    save_file_name='prep_loan.fth',\n",
    "    save_mode=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필요 없는 열 삭제 중...\n",
      "datetime으로 바꾸는 중...\n",
      "카테고리화 시키는 중...\n",
      "시간축을 기준으로 정렬 중...\n",
      "✅ prep dataset saved at (data\\prep\\prep_log.fth)\n"
     ]
    }
   ],
   "source": [
    "# Log preprocess\n",
    "log_preprocessor = Log_Preprocessor(log_df, prep_config)\n",
    "prep_log_df = log_preprocessor.run(\n",
    "    save_file_name='prep_log.fth',\n",
    "    save_mode=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime으로 바꾸는 중...\n",
      "파생변수 생성 중...\n",
      "카테고리화 시키는 중...\n",
      "원핫인코딩 중...\n",
      "순서형인코딩 중...\n",
      "✅ prep dataset saved at (data\\prep\\prep_user.fth)\n"
     ]
    }
   ],
   "source": [
    "# User preprocess\n",
    "user_preprocessor = User_Preprocessor(user_df, prep_config)\n",
    "prep_user_df = user_preprocessor.run(\n",
    "    save_file_name='prep_user.fth',\n",
    "    save_mode=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대상기간 열 나누는 중...\n",
      "datetime으로 바꾸는 중...\n",
      "시간축을 기준으로 정렬 중...\n",
      "COFIX 금리 열 이름 변경 중...\n",
      "✅ prep dataset saved at (data\\prep\\prep_cofix.fth)\n"
     ]
    }
   ],
   "source": [
    "# cofix preprocess\n",
    "cofix_preprocessor = Cofix_Preprocessor(cofix_df, prep_config)\n",
    "prep_cofix_df = cofix_preprocessor.run(\n",
    "    save_file_name='prep_cofix.fth',\n",
    "    save_mode=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher_config = config_dict.get('Matcher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(prep_user_df, prep_loan_df, prep_log_df, prep_cofix_df, matcher_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan과 Cofix 매칭중...\n",
      "Loan_Cofix와 User 매칭중...\n",
      "Train(Valid)과 Test로 나누는 중...\n",
      "✅ matched dataset saved at (data\\match\\ml_train_valid.fth)\n",
      "✅ matched dataset saved at (data\\match\\ml_test.fth)\n"
     ]
    }
   ],
   "source": [
    "train_valid, test = matcher.run(save_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TaeYeong\\Desktop\\2022_bigcontest\\2022_bigcontest\\main_test.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/TaeYeong/Desktop/2022_bigcontest/2022_bigcontest/main_test.ipynb#Y101sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m matched_total \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(train_valid,test)\n",
      "File \u001b[1;32mc:\\Users\\TaeYeong\\miniconda3\\envs\\bigcon\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TaeYeong\\miniconda3\\envs\\bigcon\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[0;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m    156\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[39m    along the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[39m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    348\u001b[0m         objs,\n\u001b[0;32m    349\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    350\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m    351\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m    352\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[0;32m    353\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[0;32m    354\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    355\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m    356\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    357\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    358\u001b[0m     )\n\u001b[0;32m    360\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\TaeYeong\\miniconda3\\envs\\bigcon\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    369\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    370\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    379\u001b[0m     sort\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    380\u001b[0m ):\n\u001b[0;32m    381\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(objs, (ABCSeries, ABCDataFrame, \u001b[39mstr\u001b[39m)):\n\u001b[1;32m--> 382\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    383\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfirst argument must be an iterable of pandas \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobjects, you passed an object of type \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(objs)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    385\u001b[0m         )\n\u001b[0;32m    387\u001b[0m     \u001b[39mif\u001b[39;00m join \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mouter\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    388\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintersect \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\""
     ]
    }
   ],
   "source": [
    "matched_total = pd.concat(train_valid, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base line\n",
    "# model_name : RF, XGB, LGBM, \n",
    "\n",
    "## 모델 서치 ##\n",
    "# from src.models import XGB\n",
    "# random_state = 42\n",
    "# xgb = XGB(train_X, train_Y, val_X, val_Y, test_X, test_Y, random_state)\n",
    "# best_model = xgb.grid_search()\n",
    "\n",
    "## test 결과 확인 및 모델 해석 ##\n",
    "# xgb.test_score(best_model)\n",
    "# xgb.confusion_matrix(best_model)\n",
    "# xgb.feature_importance(best_model)\n",
    "# xgb.shap(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.rf_model import RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanjun/opt/anaconda3/envs/bigcon/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.models.lgbm_model import LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.xgb_model import XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.catboost import CB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스케일링 진행 중...\n",
      "스케일링 전 :  (150, 4)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['application_id', 'user_id', 'insert_time', 'company_enter_month'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TaeYeong\\Desktop\\2022_bigcontest\\2022_bigcontest\\main_test.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TaeYeong/Desktop/2022_bigcontest/2022_bigcontest/main_test.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(df\u001b[39m.\u001b[39mdata, columns \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mfeature_names)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TaeYeong/Desktop/2022_bigcontest/2022_bigcontest/main_test.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m a \u001b[39m=\u001b[39m Clustering(df)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/TaeYeong/Desktop/2022_bigcontest/2022_bigcontest/main_test.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m fin_df \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TaeYeong/Desktop/2022_bigcontest/2022_bigcontest/main_test.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m fin_df\n",
      "File \u001b[1;32mc:\\Users\\TaeYeong\\Desktop\\2022_bigcontest\\2022_bigcontest\\src\\clustering.py:290\u001b[0m, in \u001b[0;36mClustering.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m) :\n\u001b[1;32m--> 290\u001b[0m     scaled_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scaling(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf)\n\u001b[0;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checking_elbows(scaled_df)\n\u001b[0;32m    294\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcluster_models[\u001b[39m'\u001b[39m\u001b[39mKM\u001b[39m\u001b[39m'\u001b[39m](scaled_df)\n",
      "File \u001b[1;32mc:\\Users\\TaeYeong\\Desktop\\2022_bigcontest\\2022_bigcontest\\src\\clustering.py:88\u001b[0m, in \u001b[0;36mClustering._scaling\u001b[1;34m(self, input_df)\u001b[0m\n\u001b[0;32m     83\u001b[0m scale_obj \u001b[39m=\u001b[39m input_df\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     86\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m스케일링 전 : \u001b[39m\u001b[39m'\u001b[39m, scale_obj\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 88\u001b[0m scale_obj \u001b[39m=\u001b[39m scale_obj\u001b[39m.\u001b[39;49mdrop(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdrop_cols, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     90\u001b[0m scale_obj \u001b[39m=\u001b[39m clean_dataset(scale_obj)\u001b[39m.\u001b[39mreset_index()\n\u001b[0;32m     93\u001b[0m ss \u001b[39m=\u001b[39m StandardScaler()\n",
      "File \u001b[1;32mc:\\Users\\TaeYeong\\miniconda3\\envs\\bigcon\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TaeYeong\\miniconda3\\envs\\bigcon\\lib\\site-packages\\pandas\\core\\frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4806\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   4807\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   4808\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4815\u001b[0m     errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   4816\u001b[0m ):\n\u001b[0;32m   4817\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4818\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4819\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4952\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4953\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4954\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   4955\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   4956\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   4957\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   4958\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   4959\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   4960\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   4961\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   4962\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\TaeYeong\\miniconda3\\envs\\bigcon\\lib\\site-packages\\pandas\\core\\generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4265\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4266\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4267\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4269\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\TaeYeong\\miniconda3\\envs\\bigcon\\lib\\site-packages\\pandas\\core\\generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   4309\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4310\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4311\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4312\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4314\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4315\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\TaeYeong\\miniconda3\\envs\\bigcon\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6642\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6643\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6644\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6645\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6646\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['application_id', 'user_id', 'insert_time', 'company_enter_month'] not found in axis\""
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from src.clustering import Clustering\n",
    "\n",
    "df = load_iris()\n",
    "df = pd.DataFrame(df.data, columns = df.feature_names)\n",
    "\n",
    "a = Clustering(df)\n",
    "fin_df = a.run()\n",
    "fin_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bigcon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5de9f3bdafa633a66c3a4ecd2a6156d7e22c318ab8aeef34d2b34d2c53bb61b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
